{"cells":[{"cell_type":"markdown","id":"93613f61-b897-46de-9afa-428e9728438a","metadata":{"id":"93613f61-b897-46de-9afa-428e9728438a"},"source":["<img src=\"data/1_1FMoK_HWvk1IBaMelUXibw.webp\" style=\"height:300px\" />"]},{"cell_type":"code","execution_count":9,"id":"0bd86a33-0ffd-48d2-9056-c7574bf13168","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"0bd86a33-0ffd-48d2-9056-c7574bf13168","executionInfo":{"status":"ok","timestamp":1760455877229,"user_tz":-420,"elapsed":19,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}},"outputId":"5746a543-e81c-4a98-97e5-c2ecf12d062b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from nltk.tokenize import sent_tokenize , word_tokenize\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","import glob\n","import re\n","import os\n","import numpy as np\n","import sys\n","Stopwords = set(stopwords.words('english'))"]},{"cell_type":"markdown","id":"47e13e23-7240-4a95-81c4-864f003c0854","metadata":{"id":"47e13e23-7240-4a95-81c4-864f003c0854"},"source":["Implementing helper functions"]},{"cell_type":"code","execution_count":10,"id":"aa0b41d0-8d2c-4235-b9ef-5fd8aeb1da27","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"aa0b41d0-8d2c-4235-b9ef-5fd8aeb1da27","executionInfo":{"status":"ok","timestamp":1760455878424,"user_tz":-420,"elapsed":52,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}},"outputId":"6f4b1c74-b3d8-42f1-9c00-445c6248917f"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:14: SyntaxWarning: invalid escape sequence '\\s'\n","<>:14: SyntaxWarning: invalid escape sequence '\\s'\n","/tmp/ipython-input-2848781559.py:14: SyntaxWarning: invalid escape sequence '\\s'\n","  regex = re.compile('[^a-zA-Z0-9\\s]')\n"]}],"source":["def finding_all_unique_words_and_freq(words):\n","    words_unique = []\n","    word_freq = {}\n","    for word in words:\n","        if word not in words_unique:\n","            words_unique.append(word)\n","    for word in words_unique:\n","        word_freq[word] = words.count(word)\n","    return word_freq\n","def finding_freq_of_word_in_doc(word,words):\n","    freq = words.count(word)\n","\n","def remove_special_characters(text):\n","    regex = re.compile('[^a-zA-Z0-9\\s]')\n","    text_returned = re.sub(regex,'',text)\n","    return text_returned"]},{"cell_type":"markdown","id":"b2f7584e-7a8e-4824-a547-0753f8e31cf4","metadata":{"id":"b2f7584e-7a8e-4824-a547-0753f8e31cf4"},"source":["Finding the set of unique words from all documents of the data set"]},{"cell_type":"code","execution_count":11,"id":"a355cc37-4092-430a-8ae8-6f2625b56acb","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"a355cc37-4092-430a-8ae8-6f2625b56acb","executionInfo":{"status":"ok","timestamp":1760455880129,"user_tz":-420,"elapsed":91,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}},"outputId":"3e79f8fa-8eaa-4399-c829-3a83dcbe5f31"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:12: SyntaxWarning: invalid escape sequence '\\d'\n","<>:12: SyntaxWarning: invalid escape sequence '\\d'\n","/tmp/ipython-input-2316475813.py:12: SyntaxWarning: invalid escape sequence '\\d'\n","  text = re.sub(re.compile('\\d'),'',text)\n"]}],"source":["all_words = []\n","dict_global = {}\n","file_folder = 'data/*'\n","idx = 1\n","files_with_index = {}\n","for file in glob.glob(file_folder):\n","    print(file)\n","    fname = file\n","    file = open(file , \"r\")\n","    text = file.read()\n","    text = remove_special_characters(text)\n","    text = re.sub(re.compile('\\d'),'',text)\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","    words = [word for word in words if len(words)>1]\n","    words = [word.lower() for word in words]\n","    words = [word for word in words if word not in Stopwords]\n","    dict_global.update(finding_all_unique_words_and_freq(words))\n","    files_with_index[idx] = os.path.basename(fname)\n","    idx = idx + 1\n","\n","unique_words_all = set(dict_global.keys())"]},{"cell_type":"markdown","id":"a6ebae2f-46dd-4e54-ad45-f21382a01cea","metadata":{"id":"a6ebae2f-46dd-4e54-ad45-f21382a01cea"},"source":["Defining the linked list"]},{"cell_type":"code","execution_count":12,"id":"3ba2475f-1013-4cbc-b9b7-007766d893a6","metadata":{"tags":[],"id":"3ba2475f-1013-4cbc-b9b7-007766d893a6","executionInfo":{"status":"ok","timestamp":1760455881512,"user_tz":-420,"elapsed":21,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}}},"outputs":[],"source":["class Node:\n","    def __init__(self ,docId, freq = None):\n","        self.freq = freq\n","        self.doc = docId\n","        self.nextval = None\n","\n","class SlinkedList:\n","    def __init__(self ,head = None):\n","        self.head = head"]},{"cell_type":"markdown","id":"c5f8513f-75df-4277-a684-a91c551ea399","metadata":{"id":"c5f8513f-75df-4277-a684-a91c551ea399"},"source":["Making a linkedlist for each word and storing all the nodes (containing the file name and frequency of the respective word ) in the linkedlist."]},{"cell_type":"code","execution_count":13,"id":"aeaea509-d82e-4679-bbfd-93d65b8d8c36","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"aeaea509-d82e-4679-bbfd-93d65b8d8c36","executionInfo":{"status":"ok","timestamp":1760455883317,"user_tz":-420,"elapsed":19,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}},"outputId":"872d8043-306e-4e24-d950-0aecfa481644"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:11: SyntaxWarning: invalid escape sequence '\\d'\n","<>:11: SyntaxWarning: invalid escape sequence '\\d'\n","/tmp/ipython-input-2238846192.py:11: SyntaxWarning: invalid escape sequence '\\d'\n","  text = re.sub(re.compile('\\d'),'',text)\n"]}],"source":["linked_list_data = {}\n","for word in unique_words_all:\n","    linked_list_data[word] = SlinkedList()\n","    linked_list_data[word].head = Node(1,Node)\n","word_freq_in_doc = {}\n","idx = 1\n","for file in glob.glob(file_folder):\n","    file = open(file, \"r\")\n","    text = file.read()\n","    text = remove_special_characters(text)\n","    text = re.sub(re.compile('\\d'),'',text)\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","    words = [word for word in words if len(words)>1]\n","    words = [word.lower() for word in words]\n","    words = [word for word in words if word not in Stopwords]\n","    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n","    for word in word_freq_in_doc.keys():\n","        linked_list = linked_list_data[word].head\n","        while linked_list.nextval is not None:\n","            linked_list = linked_list.nextval\n","        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n","    idx = idx + 1"]},{"cell_type":"markdown","id":"ed32a56b-b2d7-40b2-8885-143d5a979710","metadata":{"id":"ed32a56b-b2d7-40b2-8885-143d5a979710"},"source":["Query processing and output generation"]},{"cell_type":"code","execution_count":19,"id":"74ecadb9-7fe8-4321-a736-2d5aa213130a","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"74ecadb9-7fe8-4321-a736-2d5aa213130a","executionInfo":{"status":"ok","timestamp":1760459270821,"user_tz":-420,"elapsed":2232,"user":{"displayName":"SABRINA AMALINA","userId":"06731586952961570713"}},"outputId":"7d86fbcc-ee13-4e6f-f7e4-8610d5cc355c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your query: (badminton OR baseball) AND NOT obama\n","Operators: ['or', 'and', 'not']\n","Terms: ['(', 'badminton', 'baseball', ')', 'obama']\n","(  not found — dilewati.\n","badminton  not found — dilewati.\n","baseball  not found — dilewati.\n",")  not found — dilewati.\n","obama  not found — dilewati.\n","Tidak ada kata valid dalam query.\n"]}],"source":["query = input('Enter your query: ')\n","query = word_tokenize(query)\n","\n","connecting_words = []\n","different_words = []\n","bitwise_op = []\n","\n","# Pisahkan antara kata dan operator\n","for word in query:\n","    if word.lower() not in [\"and\", \"or\", \"not\"]:\n","        different_words.append(word.lower())\n","    else:\n","        connecting_words.append(word.lower())\n","\n","print(\"Operators:\", connecting_words)\n","print(\"Terms:\", different_words)\n","\n","total_files = len(files_with_index)\n","zeroes_and_ones_of_all_words = []\n","\n","# Buat representasi biner tiap kata\n","for word in different_words:\n","    if word.lower() in unique_words_all:\n","        zeroes_and_ones = [0] * total_files\n","        linkedlist = linked_list_data[word].head\n","        print(\"Processing:\", word)\n","        while linkedlist.nextval is not None:\n","            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n","            linkedlist = linkedlist.nextval\n","        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n","    else:\n","        print(word, \" not found — dilewati.\")\n","        # lanjut saja, jangan exit\n","        continue\n","\n","# Jika tidak ada kata yang valid\n","if len(zeroes_and_ones_of_all_words) == 0:\n","    print(\"Tidak ada kata valid dalam query.\")\n","else:\n","    # Proses operator Boolean\n","    for op in connecting_words:\n","        if len(zeroes_and_ones_of_all_words) < 2:\n","            break  # tidak cukup operand\n","\n","        word_list1 = zeroes_and_ones_of_all_words.pop(0)\n","        word_list2 = zeroes_and_ones_of_all_words.pop(0)\n","\n","        if op == \"and\":\n","            bitwise_op = [w1 & w2 for (w1, w2) in zip(word_list1, word_list2)]\n","        elif op == \"or\":\n","            bitwise_op = [w1 | w2 for (w1, w2) in zip(word_list1, word_list2)]\n","        elif op == \"not\":\n","            word_list2 = [1 - w for w in word_list2]  # NOT: ubah 1→0, 0→1\n","            bitwise_op = [w1 & w2 for (w1, w2) in zip(word_list1, word_list2)]\n","\n","        zeroes_and_ones_of_all_words.insert(0, bitwise_op)\n","\n","    # Ambil hasil akhir\n","    final_bits = zeroes_and_ones_of_all_words[0]\n","    files = []\n","\n","    for i, val in enumerate(final_bits):\n","        if val == 1:\n","            files.append(files_with_index[i + 1])  # disesuaikan offset-nya\n","\n","    print(\"Matching files:\", files)"]},{"cell_type":"code","execution_count":null,"id":"b28590aa-7b5c-407e-8420-ae808e0e4af5","metadata":{"id":"b28590aa-7b5c-407e-8420-ae808e0e4af5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}